{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1, Parameter initialization\n",
    "This notebook aims to visualize the importance of parameter initialization. The initialization step can be critical to the modelâ€™s ultimate performance, and it requires the right method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 0 - Create a simple Neural Network\n",
    "\n",
    "This task isn't focused around creating the neural network, so I'll provide a framework for creating neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        \n",
    "    def forward(self, input):\n",
    "        pass\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, input_size, output_size, init_weights, init_bias):\n",
    "        self.weights = init_weights(output_size, input_size)\n",
    "        self.bias = init_bias(output_size)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return np.dot(self.weights, self.input) + self.bias\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        # Update params and return input gradient\n",
    "        weights_gradient = np.dot(output_gradient, self.input.T)\n",
    "        self.weights -= learning_rate * weights_gradient\n",
    "        self.bias -= learning_rate * output_gradient\n",
    "        return np.dot(self.weights.T, output_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return self.activation(self.input)\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        return np.multiply(output_gradient, self.activation_prime(self.input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__(lambda x: x, lambda x: 1)\n",
    "\n",
    "class Tanh(Activation):\n",
    "    def __init__(self):\n",
    "        tanh = lambda x: np.tanh(x)\n",
    "        tanh_prime = lambda x: 1 - np.tanh(x)**2\n",
    "        super().__init__(tanh, tanh_prime)\n",
    "\n",
    "class Sigmoid(Activation):\n",
    "    def __init__(self):\n",
    "        def sigmoid(x):\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        \n",
    "        def sigmoid_prime(x):\n",
    "            s = sigmoid(x)\n",
    "            return s * (1 - s)\n",
    "            \n",
    "        super().__init__(sigmoid, sigmoid_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true - y_pred, 2))\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true) / np.size(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "### Given\n",
    "- output_size and input_size of the neural network and an example of `uniform_init`, `too_large`, `too small`\n",
    "- Xavier-initialization equation: $W^{[l]} \\sim \\mathcal{N}(\\mu=0,\\sigma^2 = \\frac{1}{n^{[l-1]}})$ where $n^{[l-1]}$ is the number of neuron in layer $l - 1$, and the biases as zero: $b^{[l]} = 0$.\n",
    "\n",
    "### Find\n",
    "- Implement the initialization methods for zeros, ones and xavier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_init(output_size, input_size = 1):\n",
    "    return np.random.uniform(-1, 1, (output_size, input_size))\n",
    "\n",
    "def too_small_init(output_size, input_size = 1):\n",
    "    return np.ones((output_size, input_size)) * 0.5\n",
    "\n",
    "def too_large_init(output_size, input_size = 1):\n",
    "    return np.ones((output_size, input_size)) * 1.5\n",
    "\n",
    "def zeros_init(output_size, input_size = 1):\n",
    "    # TODO: Implement\n",
    "    pass\n",
    "\n",
    "def ones_init(output_size, input_size = 1):\n",
    "    # TODO: Implement\n",
    "    pass\n",
    "\n",
    "def xavier_init(output_size, input_size = 1):\n",
    "    # TODO: Implement\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the neural network on XOR-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.reshape([[0,0], [0,1], [1,0], [1,1]], (4, 2, 1))\n",
    "Y = np.reshape([[0], [1], [1], [0]], (4, 1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - Impact of parameter initialization\n",
    "\n",
    "### Given\n",
    "- Methods for initializing parameters that are *zero, too small, too large, uniform or xavier*\n",
    "- XOR-dataset\n",
    "- Function for training a neural network\n",
    "- Function for visualizing the loss\n",
    "- A NN with the following architecture:\n",
    "\n",
    "| Layer | Output Shape | Activation function |\n",
    "|:-----:|:------------:|---------------------|\n",
    "| Input |    (2, 2)    |         Tanh        |\n",
    "| Dense |    (2, 2)    |         Tanh        |\n",
    "| Dense |    (2, 2)    |         Tanh        |\n",
    "| Dense |    (2, 2)    |         Tanh        |\n",
    "| Dense |    (2, 2)    |         Tanh        |\n",
    "| Dense |    (2, 2)    |         Tanh        |\n",
    "| Dense |    (2, 2)    |         Tanh        |\n",
    "| Dense |    (2, 2)    |         Tanh        |\n",
    "| Dense |    (2, 1)    |       Sigmoid       |\n",
    "\n",
    "### Find\n",
    "- The impact of this initialization by training 5 different deep neural networks, initialized with the different methods, for 20 000 epochs.\n",
    "- Visualize the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 20000\n",
    "VISUALIZE_INTERVAL = 100\n",
    "\n",
    "def create_deep_network(weight_init_method, bias_init_method):\n",
    "    return [\n",
    "        Dense(2, 2, weight_init_method, bias_init_method),\n",
    "        Tanh(),\n",
    "        Dense(2, 2, weight_init_method, bias_init_method),\n",
    "        Tanh(),\n",
    "        # TODO: Add the remaining layers/activations as described in the task\n",
    "    ]\n",
    "\n",
    "network_zero_weights =      # TODO: Call the create_deep network function with the correct parameters\n",
    "network_large_weights =     # TODO: Call the create_deep network function with the correct parameters\n",
    "network_small_weights =     # TODO: Call the create_deep network function with the correct parameters\n",
    "network_uniform_weights =   # TODO: Call the create_deep network function with the correct parameters\n",
    "network_xavier_weights =    # TODO: Call the create_deep network function with the correct parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network):\n",
    "    error_history = []\n",
    "    for epoch in range(EPOCHS):\n",
    "        error = 0\n",
    "        for x, y in zip(X, Y):\n",
    "            # Forward pass\n",
    "            output = x\n",
    "            for index, layer in enumerate(network):\n",
    "                output = layer.forward(output)\n",
    "            \n",
    "            # Calculate error\n",
    "            error += mse(y, output)\n",
    "\n",
    "            # Backward pass\n",
    "            grad = mse_prime(y, output)\n",
    "            \n",
    "            for layer in reversed(network):\n",
    "                grad = layer.backward(grad, LEARNING_RATE)\n",
    "            \n",
    "        error /= len(X)\n",
    "        error_history.append(error)\n",
    "        \n",
    "        if (epoch % VISUALIZE_INTERVAL == 0):\n",
    "            print(f\"Epoch {epoch}, error: {error}\")\n",
    "    return error_history\n",
    "\n",
    "error_history_zero_weights =    # TODO: Train the correct network using the train-function\n",
    "error_history_large_weights =   # TODO: Train the correct network using the train-function\n",
    "error_history_small_weights =   # TODO: Train the correct network using the train-function\n",
    "error_history_uniform_weights = # TODO: Train the correct network using the train-function\n",
    "error_history_xavier_weights =  # TODO: Train the correct network using the train-function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot we can see that the initialization-step has a great impact on whether the neural network converges, or not. It is especially prominent because the network we're training is deep, which increases the risk of exploding-/vanishing gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "plt.plot(error_history_zero_weights, label=\"Zero weights\")\n",
    "plt.plot(error_history_large_weights, label=\"Large weights\")\n",
    "plt.plot(error_history_small_weights, label=\"Small weights\")\n",
    "plt.plot(error_history_uniform_weights, label=\"Uniform weights\")\n",
    "plt.plot(error_history_xavier_weights, label=\"Xavier weights\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - Perks of Xavier initialization\n",
    "\n",
    "There are generally two rules of thumb when initializing parameters in a neural network.\n",
    "- The mean of the activations should be zero: $E[a^{[l-1]}] = E[a^{[l]}]$\n",
    "- The variance of the activations should stay the same across every layer: $Var(a^{[l-1]}) = Var(a^{[l]})$\n",
    "\n",
    "### Given\n",
    "- The above two rules of thumb\n",
    "- The XOR-dataset\n",
    "- A NN with the following architecture:\n",
    "\n",
    "| Layer | Output Shape | Activation function |\n",
    "|:-----:|:------------:|---------------------|\n",
    "| Input |    (2, 100)    |         Tanh        |\n",
    "| Dense |    (100, 100)    |         Tanh        |\n",
    "| Dense |    (100, 100)    |         Tanh        |\n",
    "| Dense |    (100, 100)    |         Tanh        |\n",
    "| Dense |    (100, 1)    |       Sigmoid       |\n",
    "\n",
    "### Find\n",
    "- The histograms of the activations for every 1000nd activation layer, when initialized with Xavier and Uniform\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.9\n",
    "EPOCHS = 10000\n",
    "VISUALIZE_INTERVAL = 100\n",
    "\n",
    "WEIGHT_INIT = # TODO: Choose a weight initialization method\n",
    "BIAS_INIT =   # TODO: Choose a bias initialization method\n",
    "\n",
    "network = [\n",
    "    Dense(2, 100, WEIGHT_INIT, BIAS_INIT),\n",
    "    Tanh(),\n",
    "    # TODO: Add the remaining layers/activations as described in the task\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = {}\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    error = 0\n",
    "    for x, y in zip(X, Y):\n",
    "        # Forward pass\n",
    "        output = x\n",
    "        for index, layer in enumerate(network):\n",
    "            output = layer.forward(output)\n",
    "            if (layer.__class__ == Dense):\n",
    "                key = int(index / 2)\n",
    "                if key not in activations:\n",
    "                    activations[key] = []\n",
    "                activations[key].append(output)\n",
    "        \n",
    "        # Calculate error\n",
    "        error += mse(y, output)\n",
    "\n",
    "        # Backward pass\n",
    "        grad = mse_prime(y, output)\n",
    "        \n",
    "        for layer in reversed(network):\n",
    "            grad = layer.backward(grad, LEARNING_RATE)\n",
    "        \n",
    "    error /= len(X)\n",
    "    if (epoch % VISUALIZE_INTERVAL == 0):\n",
    "        print(f\"Epoch {epoch}, error: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_number in range(0, len(activations[0]), 1000):\n",
    "    activation_data = []\n",
    "    for layer_index in range(len(activations) - 1):\n",
    "        activation_data.append(activations[layer_index][epoch_number])\n",
    "\n",
    "    utils.sns_plot_histograms(activation_data, num_bins=50, figsize=(16, 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "09dfd332e28c186c8947ed9286725e57a405257b9183302187ae9b25a18d6611"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
